{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace with 'EleutherAI/gpt-neo-125M' for GPT-Neo\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(prompt, model, tokenizer, device, max_length=500):\n",
    "    # Encode the prompt text into tokens\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Create an attention mask\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "    \n",
    "    # Generate a story with the attention mask\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length,  # Maximum length of the generated story\n",
    "        num_return_sequences=1,  # Number of stories to generate\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating phrases\n",
    "        do_sample=True,  # Use sampling to increase creativity\n",
    "        top_k=50,  # Sampling: Consider top 50 words\n",
    "        top_p=0.95,  # Sampling: Consider top probability\n",
    "        temperature=0.7,  # Creativity control\n",
    "        pad_token_id=tokenizer.eos_token_id,  # End of sequence token\n",
    "        attention_mask=attention_mask  # Pass the attention mask here\n",
    "    )\n",
    "    \n",
    "    # Decode the generated story from tokens to text\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def clean_story(story, max_line_length=80):\n",
    "    # Remove unwanted line breaks and extra spaces\n",
    "    story = story.replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n",
    "\n",
    "    # Split the story into sentences\n",
    "    sentences = re.split(r'(?<=[.!?]) +', story)\n",
    "\n",
    "    # Format the sentences into paragraphs, where each paragraph contains 3-5 sentences\n",
    "    paragraphs = []\n",
    "    paragraph = []\n",
    "    for sentence in sentences:\n",
    "        paragraph.append(sentence)\n",
    "        \n",
    "        # Add paragraph after 3-5 sentences\n",
    "        if len(paragraph) >= 3:  # You can change this number for more/less sentences per paragraph\n",
    "            paragraphs.append(\" \".join(paragraph))\n",
    "            paragraph = []\n",
    "    \n",
    "    # If there are remaining sentences, add them as the last paragraph\n",
    "    if paragraph:\n",
    "        paragraphs.append(\" \".join(paragraph))\n",
    "\n",
    "    # Join the paragraphs with a blank line between them\n",
    "    formatted_story = \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "    # Wrap each paragraph to a max line length for readability\n",
    "    wrapped_story = \"\\n\\n\".join([textwrap.fill(paragraph, width=max_line_length) for paragraph in paragraphs])\n",
    "\n",
    "    return wrapped_story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a magical forest, a young prince discovered a hidden\n",
      "treasure... The mysterious secret of a magic beast... \"A dragon.\n",
      "\n",
      "A prince.\" . â€¦ \"A prince. What did you find?\" (Malfoy's eyes widened.) , / - - -\n",
      "I'll leave you with one question: how is this possible?\n",
      "\n",
      "Well, if it's possible, then why would you do it, right? It's the least of your\n",
      "worries. .\n",
      "\n",
      ". . (I'm sure that if you're not a fan of all-knowing, it won't be the best way\n",
      "to answer this question.) I can't say that this is a coincidence.\n",
      "\n",
      "I've been looking forward to the next chapter, and I want to do something that\n",
      "will make this story more interesting and exciting. This is all the result of my\n",
      "thoughts and thoughts, not the writer's. That's why I'm asking, why is it that I\n",
      "don't know that there's a dragon in the forest?\n",
      "\n",
      "A secret. The secret that the story is about. It is not something I can reveal.\n",
      "\n",
      "If I have the right word, I might know more about it. But if there is one thing\n",
      "I know, the only thing that matters is that it is true. Even if I didn't have it\n",
      "figured out, this would be my first time reading the book.\n",
      "\n",
      "(The last question is, \"Why is there a secret?\" And it says \"I know this book is\n",
      "meant to be read and understood.\" It also says, which is an important question,\n",
      "that they were written in such a way that we cannot know it at all. Then, since\n",
      "the author's name is Malfo, you might think that that might be an issue, but it\n",
      "can also be explained as an answer to a question that was given by a friend.\n",
      "Well, there are many questions, like that of the mystery of how a person can\n",
      "have this power, though it should not be a big one.)I don  i know.\n",
      "\n",
      "When I first started reading, my eyes were blank. And then my mouth went dry. At\n",
      "first, when I read the first chapter of this series, they said, because it was a\n",
      "novel, so I had to read it all at once.\n",
      "\n",
      "They didn the 'd do a lot of reading. So I started to think about my own\n",
      "thoughts. As soon\n"
     ]
    }
   ],
   "source": [
    "# Generate and format the story\n",
    "story = generate_story(prompt, model, tokenizer, device)\n",
    "formatted_story = clean_story(story, max_line_length=80)  # Clean and format the story with wrapping\n",
    "print(formatted_story)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A robot named Max in a desert... But that's not what happened. It was a robot\n",
      "called Max that had been stolen from the warehouse and taken to the nearest\n",
      "town.\n",
      "\n",
      "\"The city was attacked by bandits, and the robot was destroyed.\" ...What did Max\n",
      "do? \"It was saved by an elderly man. He had left the town for some reason.\" The\n",
      "robot finally had the time to figure out what was going on, but he was already\n",
      "at the center of the fight.\n",
      "\n",
      "That man was Max. In the end, Max made Max's life a whole lot easier. \"Max was\n",
      "the one who saved the city and saved its citizens.\" \"We were the ones that saved\n",
      "Max, so we have to do something for him.\" Max was in his late teens.\n",
      "\n",
      "Max had already grown up around magic, he had learned how to use it, how it\n",
      "could be used to make people happy. But he still felt that he couldn't go back\n",
      "to school, because he would be an orphan. At the same time, it was not like he\n",
      "could do anything about it.\n",
      "\n",
      "Instead, his childhood was filled with troubles. This was why he found out that\n",
      "his father had died. The only thing he wanted to talk about was what he thought\n",
      "about the other man's death.\n",
      "\n",
      "After all, there were many other people who had lived through the death of their\n",
      "father. There was only one person who could save his life. And that man...\n",
      "\n",
      "He was called \"the boy who came to save the world.\" So it seemed like there was\n",
      "no one to blame but himself for Max being so stupid. Since his parents had never\n",
      "died, they were given a chance to be a hero. If there wasn't a one-on-one\n",
      "friendship between them, then they would never have died together.\n",
      "\n",
      "So, what would Max have done? He would have helped the people in the middle of\n",
      "an extremely difficult situation. When the situation was more difficult, if he\n",
      "didn't help them in some way, the only way they could have saved their life\n",
      "would've been by turning to magic.\n",
      "\n",
      "No one would understand why that would work. A simple magic spell would not work\n",
      "if there weren't any other magic spells in existence. They would only learn to\n",
      "cast it in order to survive.\n",
      "\n",
      "For their entire lives, that meant that they had to learn about magic to perform\n",
      "magic magic tricks. Not even that. Even if they knew the right magic trick to\n",
      "work, all they wanted\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A robot named Max in a desert...\"\n",
    "\n",
    "# Generate the story\n",
    "story = generate_story(prompt, model, tokenizer, device)\n",
    "\n",
    "formatted_story = clean_story(story, max_line_length=80)  # Clean and format the story with wrapping\n",
    "print(formatted_story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
